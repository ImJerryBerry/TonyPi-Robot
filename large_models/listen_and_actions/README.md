# 语音大模型控制动作 (Listen and Actions)

## 功能简介
语音大模型控制动作模块利用大语言模型来实现自然语言指令到机器人动作的转换。
用户可以通过自然语言描述想要机器人执行的动作序列，系统将语音转换为文本，通过大模型理解意图，并生成结构化的动作指令，最终控制机器人执行复杂的动作组合。

## 使用方法
1. **启动脚本**：运行`listen2actions.py`
2. **描述动作**：用自然语言描述想要机器人执行的动作序列，例如：“向前走，然后举起双手，最后鞠躬”
3. **执行动作**：系统将解析指令，并控制机器人执行相应动作

提示：正式使用前需要在代码内设置好API-KEY。

## 技术实现
- **语音识别**：
  - 使用讯飞云端语音识别服务
  - 将用户语音指令转换为文本形式

- **大语言模型解析**：
  - 使用阿里云通义千问大语言模型
  - 设计专门的提示词引导模型理解动作指令
  - 将自然语言解析为结构化的JSON格式动作序列

- **动作映射与执行**：
  - 将大模型输出的动作ID映射到机器人预设的动作组
  - 按照序列顺序依次执行动作
  - 提供动作执行的反馈和状态播报

- **语音反馈**：
  - 使用云端TTS提供全程语音引导
  - 播报当前执行的动作和执行状态

## 系统组件
- `llm_client.py`：大语言模型客户端，负责与阿里云通义千问API通信
- `json2actions.py`：解析大模型返回的JSON动作指令，并执行对应动作
- `ActionGroupDict.py`：动作组映射字典，定义了可用的动作ID和名称

## 动作能力
系统可以执行TonyPi机器人支持的所有预设动作，具体动作请参见动作字典文件`ActionGroupDict.py`。

## 注意事项
- 使用此功能前请确保机器人插上了扬声器和麦克风，仅使用ASR和TTS模块无法实现此功能
- API调用可能产生费用，请关注API使用量
- 描述动作时尽量使用简洁明了的语言
- 确保机器人有足够的活动空间执行完整动作序列