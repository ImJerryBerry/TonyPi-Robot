# 视觉大模型场景识别 (VLM)

## 功能简介
视觉大模型场景识别模块利用通义千问VL-Max视觉语言大模型，实现对机器人摄像头拍摄景象的智能理解和描述。
用户通过语音指令触发拍照，系统将图像发送至大模型进行分析，并通过语音合成技术播报分析结果。

## 使用方法
1. **启动脚本**：运行`VLM.py`
2. **激活语音识别**：系统默认开启循环模式，可直接说出第3步的指令
3. **发出检测指令**：说出“这是什么”
4. **获取结果**：系统将拍照并通过语音播报大模型的分析结果

提示：正式使用前需要在代码内设置好API-KEY

## 模式切换
本脚本默认使用循环模式，每次说出指令前不需要先说激活词。设置为口令模式后，每次说出指令前需要先说出激活词“开始”。

如果需要切换至口令模式，请修改代码为：

    asr.setMode(2)

## 技术实现
- **视觉语言大模型**：
  - 使用阿里云通义千问视觉语言大模型Qwen-VL-Max
  - 通过DashScope API发送图像并获取分析结果
  - 模型具有强大的场景理解和细节描述能力

- **图像捕获**：
  - 使用OpenCV库捕获高质量图像
  - 将图像保存为本地文件，便于API调用

- **语音交互**：
  - 使用ASR模块处理语音指令
  - 使用TTS模块播报分析结果
  - 实现智能分段播报，避免了TTS无法朗读长文本的问题

## API配置
- 使用阿里云DashScope API
- 模型：qwen-vl-max-latest
- 正式使用前需要在代码内设置好API-KEY

## 核心功能
- **场景理解**：描述拍摄场景的整体内容
- **物体识别**：识别画面中的各类物体
- **细节分析**：分析场景细节和物体间关系
- **自然语言生成**：将视觉信息转化为自然、流畅的中文描述

## 常见问题
如果机器人在回答时，控制台输出`Sensor not connected!`，且该分段语音未能成功播放，说明该段文本较长导致TTS模块无法朗读。
可在调用大模型时手动限制每个分段的文本长度来解决此问题（默认限制为每段文本最多8个字）。

## 注意事项
- API调用可能产生费用，请关注API使用量
- 光照条件会影响图像质量和识别效果
- 模型分析需要一定时间，请耐心等待结果 